stages:
  preprocess:
    cmd: >
      python process_dataset.py 
      --data_size 1000
      --seed 42
      --response_tokens_list 0,1,2,3,4,5
    deps:
      - process_dataset.py

  generate_phi_responses:
    cmd: >
      python generate_phi_responses.py
      --data_size 10
      --batch_size 128
      --seed 42
      --output_dir ./data
      --max_length 512
    deps:
      - generate_phi_responses.py

  train_length_predictors:
    foreach:
      - 0
      - 2
      - 5
      - 10
    do:
      cmd: >
        python lenght_prediction.py
        --data_dir data/lmsys_vicuna-13b_preview${item}_1000K
        --model_name google-bert/bert-base-uncased
        --output_dir results/bert_length_predictor_${item}
        --num_epochs 40
        --batch_size 30
        --learning_rate 1e-5
        --weight_decay 0.01
        --warmup_ratio 0.1
        --early_stopping_patience 2
        --seed 42
        --use_wandb
        --wandb_project output-length-prediction
        --do_train
        --do_eval
        --max_gen_tokens ${item}
      deps:
        - lenght_prediction.py
        - models/BasicBert.py

