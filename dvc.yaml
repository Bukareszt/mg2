stages:
  preprocess:
    cmd: >
      python process_dataset.py 
      --data_size 1000
      --seed 42
      --response_tokens_list 0,1,2,3,4,5
    deps:
      - process_dataset.py

  process_for_embeddings:
    cmd: >
      python embedings_preprocess.py
      --data_size 10
      --seed 42
      --llama_model_name meta-llama/Llama-3.1-8B-Instruct
      --output_dir ./data
      --batch_size 4
      --max_new_tokens 8192
      --hf_token $HF_TOKEN
      
  train_embeddings_predictors:
    foreach:
      # - "mean"
      # - "max"
      - "concat"
    do:
      cmd: >
        python embedings_prediction.py
        --data_dir data/lmsys_vicuna-13b_preview0_10K
        --output_dir results/embeddings_predictor_${item}
        --do_train
        --do_eval
        --use_wandb
        --wandb_project output-length-prediction
        --aggregation ${item}
      deps:
        - embedings_prediction.py

  train_length_predictors:
    foreach:
      - 0
      - 1
      - 2
      - 3
      - 4
      - 5
    do:
      cmd: >
        python lenght_prediction.py
        --data_dir data/lmsys_vicuna-13b_preview${item}_1000K
        --model_name google-bert/bert-base-uncased
        --output_dir results/bert_length_predictor_${item}
        --num_epochs 40
        --batch_size 30
        --learning_rate 1e-5
        --weight_decay 0.01
        --warmup_ratio 0.1
        --early_stopping_patience 2
        --seed 42
        --use_wandb
        --wandb_project output-length-prediction
        --do_train
        --do_eval
        --max_gen_tokens ${item}
      deps:
        - lenght_prediction.py
        - models/BasicBert.py

